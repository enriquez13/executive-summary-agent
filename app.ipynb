{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Alejo\\Analisis de Datos\\executive-summary-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING EXECUTIVE SUMMARY AGENT\n",
      "============================================================\n",
      "Loading PDF with pdfplumber: annual-report-adidas-ar24.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P7494' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P7494' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted: 1,422,572 characters\n",
      "Text after cleaning: 1,282,748 characters\n",
      "Reduction: 139,824 deleted characters\n",
      "Dividing text into chunks (size=4000, overlap=500)...\n",
      "Text divided into 366 chunk(s).\n",
      "Creating embeddings and FAISS index for 366 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14948\\3315227164.py:101: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = SentenceTransformerEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 366 vectors.\n",
      "\n",
      "Searching for specific financial content...\n",
      "Recovering 10 chunks financial...\n",
      "Best chunk score: 21\n",
      "[Chunks initially recovered: 10\n",
      "\n",
      "Filtering chunks by executable value...\n",
      "[FILTER] Top 3 scores: [100, 50, 50]\n",
      "[FILTER] 5/10 chunks selected\n",
      "Final executive chunks: 5\n",
      "\n",
      "Generating executive summary...\n",
      "Initializing Groq's LLM...\n",
      "Generating executive summary with LLM...\n",
      "\n",
      "============================================================\n",
      "FINANCIAL EXECUTIVE SUMMARY\n",
      "============================================================\n",
      "---\n",
      "FINANCIAL EXECUTIVE SUMMARY\n",
      "---\n",
      "\n",
      "üéØ KEY RESULTS (TOP 5)\n",
      "‚Ä¢ Net sales: ‚Ç¨23,683 million (2024) vs ‚Ç¨21,427 million (2023) (11% increase)\n",
      "‚Ä¢ Operating profit: ‚Ç¨1,337 million (2024) vs ‚Ç¨268 million (2023) (398% increase)\n",
      "‚Ä¢ Gross margin: 50.8% (2024) vs 47.5% (2023) (3.3 percentage points increase)\n",
      "‚Ä¢ Average operating working capital as a percentage of sales: 19.7% (2024) vs 25.7% (2023) (5.9 percentage points decrease)\n",
      "‚Ä¢ EBITDA: ‚Ç¨2,465 million (2024) vs ‚Ç¨1,358 million (2023) (81% increase)\n",
      "\n",
      "üìä DETAILED ANALYSIS\n",
      "1. Profitability: Operating profit reached ‚Ç¨1,337 million, with an operating margin of 5.6%, and net income from continuing operations was ‚Ç¨824 million.\n",
      "2. Sales/Revenue: Net sales increased by 11% to ‚Ç¨23,683 million, with a currency-neutral growth rate of 12%.\n",
      "3. Efficiency: Average operating working capital as a percentage of sales decreased to 19.7%, and capital expenditure was ‚Ç¨540 million.\n",
      "4. Outlook: The company expects currency-neutral sales to increase at a high-single-digit rate in 2025, with operating profit projected to be between ‚Ç¨1.7 billion and ‚Ç¨1.8 billion.\n",
      "\n",
      "‚ö†Ô∏è RISKS/OPPORTUNITIES\n",
      "‚Ä¢ Risk 1: Macroeconomic challenges and geopolitical tensions may negatively affect consumer sentiment and discretionary spending power.\n",
      "‚Ä¢ Risk 2: NOT IDENTIFIED IN DOCUMENT.\n",
      "‚Ä¢ Risk 3: NOT IDENTIFIED IN DOCUMENT.\n",
      "‚Ä¢ Opportunity 1: The company expects to gain further market share and grow currency-neutral sales at a high-single-digit rate in 2025.\n",
      "‚Ä¢ Opportunity 2: The global sporting goods industry is set to continue its positive development.\n",
      "‚Ä¢ Opportunity 3: NOT IDENTIFIED IN DOCUMENT.\n",
      "\n",
      "üí° EXECUTIVE RECOMMENDATION\n",
      "The company is well-positioned for future growth, with a strong brand momentum and a solid financial foundation. We recommend continuing to focus on profitable growth and operating leverage to achieve the projected operating profit of between ‚Ç¨1.7 billion and ‚Ç¨1.8 billion in 2025.\n",
      "\n",
      "============================================================\n",
      "Summary saved in: executive_summary.txt\n",
      "Executive chunks saved in: chunks_executives.txt\n",
      "\n",
      "============================================================\n",
      "PROCESS METRICS\n",
      "============================================================\n",
      "‚Ä¢ Original text: 1,422,572 characters\n",
      "‚Ä¢ Clean text: 1,282,748 characters\n",
      "‚Ä¢ Generated chunks: 366\n",
      "‚Ä¢ Recovered financial chunks: 10\n",
      "‚Ä¢ Filtered executive chunks: 5\n",
      "‚Ä¢ Data reduction: 9.8%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Executive Summary Agent - MVP\n",
    "Author: Alejandro\n",
    "Objective: Demonstrate an advanced RAG flow for summarizing financial PDF documents.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- LangChain and RAG Components ---\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. DOCUMENT PROCESSING COMPONENTS\n",
    "# --------------------------------------------------------------------\n",
    "def load_pdf(file_path: str) -> str:\n",
    "    \"\"\"Load a PDF and extract its text using pdfplumber.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"[INFO] Loading PDF with pdfplumber: {file_path}\")\n",
    "    extracted_text = \"\"  # Changed from texto_extraido\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                extracted_text += page.extract_text() + \"\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Improved cleaning for headers.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    # Specific pattern for repetitive Adidas headers\n",
    "    adidas_header_pattern = r'^TO OUR GROUP MANAGEMENT REPORT ‚Äì GROUP MANAGEMENT REPORT ‚Äì GROUP MANAGEMENT REPORT ‚Äì CONSOLIDATED ADDITIONAL$'\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # 1. Remove the exact repetitive header\n",
    "        if re.match(adidas_header_pattern, stripped):\n",
    "            continue\n",
    "            \n",
    "        # 2. Remove the section line (appears below the header)\n",
    "        if stripped == \"SHAREHOLDERS OUR COMPANY FINANCIAL REVIEW SUSTAINABILITY STATEMENT FINANCIAL STATEMENTS INFORMATION\":\n",
    "            continue\n",
    "            \n",
    "        # 3. Remove single numeric lines\n",
    "        if re.fullmatch(r'(\\d\\s+)+\\d', stripped):\n",
    "            continue\n",
    "            \n",
    "        # 4. Keep lines with substantial content\n",
    "        if len(stripped) > 20:\n",
    "            cleaned_lines.append(stripped)\n",
    "    \n",
    "    result = '\\n'.join(cleaned_lines)\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 4000, chunk_overlap: int = 500) -> list[str]:\n",
    "    \"\"\"Divide the text into overlapping fragments using LangChain.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    print(f\"[INFO] Dividing text into chunks (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. RAG COMPONENTS - EMBEDDINGS, VECTORSTORE, SEARCH\n",
    "# --------------------------------------------------------------------\n",
    "def create_and_index_vectorstore(chunks: list[str]) -> FAISS:\n",
    "    \"\"\"Create embeddings and build a FAISS index.\"\"\"\n",
    "    print(f\"[INFO] Creating embeddings and FAISS index for {len(chunks)} chunks...\")\n",
    "    \n",
    "    embeddings_model = SentenceTransformerEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    vectorstore = FAISS.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embeddings_model\n",
    "    )\n",
    "    \n",
    "    print(f\"[INFO] Vectorstore created with {vectorstore.index.ntotal} vectors.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def retrieve_financial_chunks(vectorstore: FAISS, query: str, top_k: int = 10) -> list[str]:\n",
    "    \"\"\"Retrieve chunks with a focus on financial content using hybrid search.\"\"\"\n",
    "    print(f\"[INFO] Retrieving {top_k} financial chunks...\")  # Fixed message\n",
    "    \n",
    "    # Semantic similarity search\n",
    "    semantic_docs = vectorstore.similarity_search(query, k=top_k * 2)\n",
    "    \n",
    "    # Filter by financial keywords\n",
    "    financial_keywords = [\n",
    "        'revenue', 'income', 'profit', 'ebitda', 'margin', \n",
    "        'cash flow', 'balance sheet', 'financial statement',\n",
    "        'euro', 'million', 'billion', '%', 'growth',\n",
    "        'sales', 'net income', 'operating', 'segment',\n",
    "        'quarter', 'annual', 'forecast', 'guidance'\n",
    "    ]\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in semantic_docs:\n",
    "        content_lower = doc.page_content.lower()\n",
    "        score = 0\n",
    "        \n",
    "        # Score by financial keywords\n",
    "        for keyword in financial_keywords:\n",
    "            if keyword in content_lower:\n",
    "                score += 1\n",
    "        \n",
    "        # Bonus for numbers with decimals (likely financial figures)\n",
    "        if re.search(r'\\d+[\\.,]\\d+', doc.page_content):\n",
    "            score += 2\n",
    "            \n",
    "        # Bonus for currency symbols\n",
    "        if re.search(r'[‚Ç¨$\\¬£]', doc.page_content):\n",
    "            score += 3\n",
    "            \n",
    "        # Bonus for percentages\n",
    "        if re.search(r'\\d+\\s*%', doc.page_content):\n",
    "            score += 2\n",
    "            \n",
    "        scored_docs.append((score, doc))\n",
    "    \n",
    "    # Sort by score and take the best ones\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_docs = [doc for _, doc in scored_docs[:top_k]]\n",
    "    \n",
    "    if scored_docs:\n",
    "        print(f\"[DEBUG] Best chunk score: {scored_docs[0][0]}\")\n",
    "    \n",
    "    return [doc.page_content for doc in best_docs]\n",
    "\n",
    "\n",
    "def filter_executive_chunks(all_chunks: list[str], min_financial_score: int = 20, max_chunks: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filter chunks to keep only those with high executive value.\n",
    "    \"\"\"\n",
    "    scored = []\n",
    "    \n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        score = 0\n",
    "        \n",
    "        # High points for financial tables\n",
    "        if re.search(r'\\d{4}\\s+\\d{4}\\s+\\d{4}\\s+\\d{4}', chunk):  # Table pattern\n",
    "            score += 50\n",
    "        \n",
    "        # Points for financial figures in ‚Ç¨\n",
    "        if re.search(r'‚Ç¨\\s*\\d+[\\.,]\\d+', chunk):\n",
    "            score += 30\n",
    "            \n",
    "        # Points for percentages\n",
    "        if re.search(r'\\d+\\.?\\d*\\s*%', chunk):\n",
    "            score += 20\n",
    "            \n",
    "        # Penalize residual headers\n",
    "        if 'TO OUR GROUP MANAGEMENT REPORT' in chunk:\n",
    "            score -= 40\n",
    "            \n",
    "        scored.append((score, chunk))\n",
    "    \n",
    "    # Order by score and take the best\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_chunks = [chunk for score, chunk in scored if score >= min_financial_score][:max_chunks]\n",
    "    \n",
    "    print(f\"[FILTER] Top 3 scores: {[s for s, _ in scored[:3]]}\")\n",
    "    print(f\"[FILTER] {len(best_chunks)}/{len(all_chunks)} chunks selected\")\n",
    "    \n",
    "    return best_chunks\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. LLM COMPONENT - SUMMARY GENERATION\n",
    "# --------------------------------------------------------------------\n",
    "def initialize_groq_llm(model_name: str = \"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"Initialize the Groq client.\"\"\"\n",
    "    import os\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"ERROR: GROQ_API_KEY not found.\"  # Fixed error message\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            groq_api_key=api_key,\n",
    "            model_name=model_name,\n",
    "            temperature=0.2,  # Low temperature for precision\n",
    "            max_tokens=1500   # Enough for an executive summary\n",
    "        )\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Error with model {model_name}: {e}\")\n",
    "        print(\"[WARNING] Trying alternative model 'mixtral-8x7b-32768'...\")\n",
    "        return ChatGroq(\n",
    "            groq_api_key=api_key,\n",
    "            model_name=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_executive_summary(retrieved_chunks: list[str], query: str) -> str:\n",
    "    \"\"\"Generate an executive summary using LangChain with Groq.\"\"\"\n",
    "    print(\"[INFO] Initializing Groq LLM...\")\n",
    "    llm = initialize_groq_llm()\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are the CFO of a consulting firm, preparing an URGENT executive briefing for the CEO.\n",
    "\n",
    "ABSOLUTE RULES:\n",
    "1. Use ONLY the information provided in the context. DO NOT invent data.\n",
    "2. Focus on specific numerical data: figures in millions/billions, percentages, growth rates.\n",
    "3. Extract and present the most important data from the financial tables.\n",
    "4. MANDATORY structure:\n",
    "   ---\n",
    "   FINANCIAL EXECUTIVE SUMMARY\n",
    "   ---\n",
    "   \n",
    "   üéØ KEY RESULTS (TOP 5)\n",
    "   ‚Ä¢ [Metric 1]: [2024 value] vs [2023 value] ([% change] if available)\n",
    "   ‚Ä¢ [Metric 2]: [2024 value] vs [2023 value] ([% change] if available)\n",
    "   ‚Ä¢ ... (maximum 5 points)\n",
    "   \n",
    "   üìä DETAILED ANALYSIS\n",
    "   1. Profitability: [Operating profit, net income, margins]\n",
    "   2. Sales/Revenue: [Revenue, sales, segments]\n",
    "   3. Efficiency: [Cash flow, working capital, ratios]\n",
    "   4. Outlook: [Any projections or guidance mentioned]\n",
    "   \n",
    "   ‚ö†Ô∏è RISKS/OPORTUNITIES (maximum 3 of each, only if mentioned in the document)\n",
    "   ‚Ä¢ [Risk 1]: [Brief explanation]\n",
    "   ‚Ä¢ [Opportunity 1]: [Brief explanation]\n",
    "   (If no risks/opportunities are mentioned, omit this entire section)\n",
    "   \n",
    "   üí° EXECUTIVE RECOMMENDATION (1-2 sentences)\n",
    "\n",
    "5. ALWAYS include units (‚Ç¨ million, %, etc.).\n",
    "6. If a section has no data in the context, OMIT IT ENTIRELY (do not write \"NOT IDENTIFIED\").\n",
    "7. Use emojis to improve readability.\n",
    "8. Maximum 400 words.\n",
    "9. Respond in ENGLISH.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"RAW CONTEXT EXTRACTED FROM THE ANNUAL REPORT:\n",
    "{context}\n",
    "\n",
    "--- \n",
    "GENERATE THE EXECUTIVE SUMMARY STRICTLY FOLLOWING THE ABOVE RULES.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = (\n",
    "        {\"context\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"[INFO] Generating executive summary with LLM... (this may take 15-30 seconds)\")\n",
    "    \n",
    "    try:\n",
    "        summary = chain.invoke(context)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to generate summary: {e}\")\n",
    "        return f\"\"\"\n",
    "        [ERROR] The executive summary could not be generated automatically.\n",
    "        Reason: {e}\n",
    "        \n",
    "        Retrieved chunks: {len(retrieved_chunks)}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. MAIN FLOW\n",
    "# --------------------------------------------------------------------\n",
    "def main(pdf_path: str):\n",
    "    \"\"\"End-to-end executive summary agent flow.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXECUTIVE SUMMARY AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load document\n",
    "    raw_text = load_pdf(pdf_path)\n",
    "    if not raw_text:\n",
    "        print(\"[ERROR] Failed to extract text from the PDF.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[INFO] Extracted text: {len(raw_text):,} characters\")\n",
    "    \n",
    "    # 2. Clean text\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    print(f\"[INFO] Cleaned text: {len(cleaned_text):,} characters\")\n",
    "    print(f\"[INFO] Reduction: {len(raw_text)-len(cleaned_text):,} characters removed\")\n",
    "    \n",
    "    if len(cleaned_text) < 50000:\n",
    "        print(\"[WARNING] Cleaned text is very short. Verify cleaning process.\")\n",
    "    \n",
    "    # 3. Split into chunks\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f\"[INFO] Text split into {len(chunks)} chunks.\")\n",
    "    \n",
    "    if len(chunks) == 0:\n",
    "        print(\"[ERROR] No chunks generated.\")\n",
    "        return\n",
    "    \n",
    "    # 4. Create vectorstore (embeddings + FAISS)\n",
    "    vectorstore = create_and_index_vectorstore(chunks)\n",
    "    \n",
    "    # 5. SPECIFIC FINANCIAL QUERY (Completely in English)\n",
    "    financial_query = \"\"\"\n",
    "    Financial results 2024 vs 2023: \n",
    "    Operating profit, Revenue, Sales, Net income, EBITDA, \n",
    "    Gross margin, Operating margin, Cash flow, \n",
    "    Segment performance (Footwear, Apparel, Accessories),\n",
    "    Regional results (North America, EMEA, Asia-Pacific),\n",
    "    Financial guidance 2025,\n",
    "    Risks mentioned, Opportunities mentioned.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 6. Retrieve financial chunks\n",
    "    print(\"\\n[INFO] Searching for specific financial content...\")\n",
    "    financial_chunks = retrieve_financial_chunks(vectorstore, financial_query, top_k=10)\n",
    "    print(f\"[INFO] Initially retrieved {len(financial_chunks)} financial chunks.\")  # Fixed message\n",
    "    \n",
    "    # 7. Filter by executive value\n",
    "    print(\"\\n[INFO] Filtering chunks by executive value...\")\n",
    "    executive_chunks = filter_executive_chunks(financial_chunks, min_financial_score=20, max_chunks=5)\n",
    "    \n",
    "    if not executive_chunks:\n",
    "        print(\"[WARNING] Filter too strict. Using top 3 chunks.\")\n",
    "        executive_chunks = financial_chunks[:3]\n",
    "    \n",
    "    print(f\"[INFO] Final executive chunks: {len(executive_chunks)}\")\n",
    "    \n",
    "    # 8. Generate executive summary\n",
    "    print(\"\\n[INFO] Generating executive summary...\")\n",
    "    summary = generate_executive_summary(executive_chunks, financial_query)\n",
    "    \n",
    "    # 9. Present results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINANCIAL EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(summary)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # 10. Save results\n",
    "    output_path = \"executive_summary.txt\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"FINANCIAL EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(f\"[INFO] Summary saved to: {output_path}\")\n",
    "    \n",
    "    # 11. Save executive chunks for debugging\n",
    "    debug_path = \"executive_chunks_debug.txt\"\n",
    "    with open(debug_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(executive_chunks):\n",
    "            f.write(f\"\\n{'='*50}\\nEXECUTIVE CHUNK #{i+1}\\n{'='*50}\\n\")\n",
    "            f.write(chunk[:800] + (\"...\" if len(chunk) > 800 else \"\"))\n",
    "            f.write(f\"\\n\\n[Total length: {len(chunk):,} characters]\")\n",
    "    \n",
    "    print(f\"[DEBUG] Executive chunks saved to: {debug_path}\")\n",
    "    \n",
    "    # 12. Final metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESS METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚Ä¢ Original text: {len(raw_text):,} characters\")\n",
    "    print(f\"‚Ä¢ Cleaned text: {len(cleaned_text):,} characters\")\n",
    "    print(f\"‚Ä¢ Generated chunks: {len(chunks)}\")\n",
    "    print(f\"‚Ä¢ Retrieved financial chunks: {len(financial_chunks)}\")\n",
    "    print(f\"‚Ä¢ Filtered executive chunks: {len(executive_chunks)}\")\n",
    "    print(f\"‚Ä¢ Data reduction: {(len(raw_text)-len(cleaned_text))/len(raw_text)*100:.1f}%\")\n",
    "    \n",
    "    # Optional: Save cleaned text for inspection\n",
    "    with open(\"cleaned_text_debug.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text[:5000] + \"\\n\\n[...]\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. ENTRY POINT\n",
    "# --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"annual-report-adidas-ar24.pdf\"\n",
    "    \n",
    "    if not Path(pdf_path).exists():\n",
    "        print(f\"[ERROR] PDF not found: {pdf_path}\")\n",
    "        print(\"[INFO] Creating a test document...\")\n",
    "        \n",
    "        # Create a simple test document\n",
    "        test_text = \"\"\"\n",
    "        ADIDAS ANNUAL REPORT 2024 - KEY FINANCIAL HIGHLIGHTS\n",
    "        \n",
    "        Operating Results:\n",
    "        ‚Ä¢ Revenue: ‚Ç¨23.5 billion (2024) vs ‚Ç¨21.9 billion (2023) - +7.3% growth\n",
    "        ‚Ä¢ Operating Profit: ‚Ç¨1.8 billion (2024) vs ‚Ç¨1.2 billion (2023) - +50% improvement\n",
    "        ‚Ä¢ Net Income: ‚Ç¨1.2 billion (2024) vs ‚Ç¨0.8 billion (2023) - +50% growth\n",
    "        ‚Ä¢ EBITDA Margin: 12.5% (2024) vs 10.8% (2023) - +170 bps improvement\n",
    "        \n",
    "        Segment Performance:\n",
    "        ‚Ä¢ Footwear: ‚Ç¨15.2 billion (+8% YoY)\n",
    "        ‚Ä¢ Apparel: ‚Ç¨7.1 billion (+6% YoY)\n",
    "        ‚Ä¢ Accessories: ‚Ç¨1.2 billion (+5% YoY)\n",
    "        \n",
    "        Regional Performance:\n",
    "        ‚Ä¢ North America: ‚Ç¨8.5 billion (+9% YoY)\n",
    "        ‚Ä¢ EMEA: ‚Ç¨7.8 billion (+6% YoY)\n",
    "        ‚Ä¢ Asia-Pacific: ‚Ç¨5.9 billion (+8% YoY)\n",
    "        \n",
    "        Cash Flow & Balance Sheet:\n",
    "        ‚Ä¢ Free Cash Flow: ‚Ç¨1.5 billion\n",
    "        ‚Ä¢ Net Debt: ‚Ç¨2.1 billion (improved from ‚Ç¨2.8 billion in 2023)\n",
    "        ‚Ä¢ Dividend per share: ‚Ç¨2.00 (2024) vs ‚Ç¨1.50 (2023)\n",
    "        \n",
    "        2025 Guidance:\n",
    "        ‚Ä¢ Revenue growth: 5-7%\n",
    "        ‚Ä¢ Operating margin: 11-12%\n",
    "        ‚Ä¢ EPS growth: 10-12%\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(\"test_document.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(test_text)\n",
    "        pdf_path = \"test_document.txt\"\n",
    "    \n",
    "    try:\n",
    "        main(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
